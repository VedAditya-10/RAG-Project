{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c25e6cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 2 text chunks from c:\\Users\\Vedaditya\\Desktop\\CollegeGPT\\RAG_P1\\data\\Syllabus.docx to the Chroma collection.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import chromadb\n",
    "from docx import Document\n",
    "\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "DATA_PATH = os.path.join(BASE_DIR, \"data\")\n",
    "DOC_FILE = os.path.join(DATA_PATH, \"Syllabus.docx\")\n",
    "\n",
    "\n",
    "# extract text\n",
    "\n",
    "def extract_text_from_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    full_text = \"\"\n",
    "    for para in doc.paragraphs:\n",
    "        full_text += para.text + \"\\n\"\n",
    "    return full_text.strip()\n",
    "\n",
    "\n",
    "text = extract_text_from_docx(DOC_FILE)\n",
    "\n",
    "if not text:\n",
    "    print(\"No text extracted! Check if the Word document contains readable text.\")\n",
    "    exit()\n",
    "\n",
    "# Split text into chunks for RAG\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "texts = text_splitter.split_text(text)\n",
    "\n",
    "# ChromaDB\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection_name = \"syllabus_collection\"\n",
    "\n",
    "try:\n",
    "    collection = client.get_collection(name=collection_name)\n",
    "except:\n",
    "    collection = client.create_collection(name=collection_name)\n",
    "\n",
    "for i, chunk in enumerate(texts):\n",
    "    collection.add(\n",
    "        documents=[chunk],\n",
    "        metadatas=[{\"source\": f\"{DOC_FILE}_chunk_{i}\"}],\n",
    "        ids=[f\"{DOC_FILE}_chunk_{i}\"]\n",
    "    )\n",
    "\n",
    "print(f\"Added {len(texts)} text chunks from {DOC_FILE} to the Chroma collection.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99e322d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1345 characters from Syllabus.docx\n",
      "Added 2 chunks to ChromaDB\n",
      "Retrieved relevant chunks from syllabus\n",
      "Answer:\n",
      "---------------------\n",
      "Here are the subjects mentioned in the syllabus:\n",
      "\n",
      "For 5th Semester (Third Year):\n",
      "\n",
      "1. Theory of Computation\n",
      "2. Operating Systems\n",
      "3. Artificial Intelligence and Machine Learning\n",
      "4. Computational Intelligence\n",
      "5. Data Mining and Data Warehousing\n",
      "6. Microprocessors and Microcontrollers\n",
      "7. Distributed Systems\n",
      "8. Cryptographic Foundation and Network Security\n",
      "9. Object-Oriented Analysis and Design\n",
      "10. Web Technology\n",
      "\n",
      "For 6th Semester (Third Year):\n",
      "\n",
      "1. Compiler Design\n",
      "2. Software Engineering\n",
      "3. Evolutionary Computing\n",
      "4. Pattern Recognition\n",
      "5. Embedded Systems\n",
      "6. Cloud and Edge Computing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import chromadb\n",
    "from docx import Document\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "DATA_PATH = os.path.join(BASE_DIR, \"data\")\n",
    "DOC_FILE = os.path.join(DATA_PATH, \"Syllabus.docx\")\n",
    "\n",
    "doc = Document(DOC_FILE)\n",
    "full_text = \"\"\n",
    "for para in doc.paragraphs:\n",
    "    full_text += para.text + \"\\n\"\n",
    "text = full_text.strip()\n",
    "\n",
    "print(f\"Extracted {len(text)} characters from Syllabus.docx\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "texts = text_splitter.split_text(text)\n",
    "\n",
    "# ChromaDB setup\n",
    "client = chromadb.Client()\n",
    "collection_name = \"syllabus_collection\"\n",
    "\n",
    "try:\n",
    "    client.delete_collection(name=collection_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "collection = client.create_collection(name=collection_name)\n",
    "\n",
    "for i, chunk in enumerate(texts):\n",
    "    collection.add(\n",
    "        documents=[chunk],\n",
    "        metadatas=[{\"source\": f\"syllabus_chunk_{i}\"}],\n",
    "        ids=[f\"chunk_{i}\"]\n",
    "    )\n",
    "\n",
    "print(f\"Added {len(texts)} chunks to ChromaDB\")\n",
    "\n",
    "# user for query\n",
    "user_query = input(\"\\nWhat do you want to know about the syllabus? \")\n",
    "\n",
    "# Retrieve from ChromaDB\n",
    "results = collection.query(\n",
    "    query_texts=[user_query],\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "print(\"Retrieved relevant chunks from syllabus\")\n",
    "\n",
    "# Ollama\n",
    "prompt = f\"\"\"You are a helpful assistant. Answer questions about the Computer Science Engineering Department syllabus.\n",
    "Answer only based on the syllabus content below. If you don't know, say \"I don't know\".\n",
    "\n",
    "Syllabus content:\n",
    "{results['documents'][0]}\n",
    "\n",
    "Question: {user_query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Ollama API\n",
    "response = requests.post('http://localhost:11434/api/generate',\n",
    "                        json={\n",
    "                            'model': 'llama3.2',\n",
    "                            'prompt': prompt,\n",
    "                            'stream': False\n",
    "                        })\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(\"---------------------\")\n",
    "print(response.json()['response'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
